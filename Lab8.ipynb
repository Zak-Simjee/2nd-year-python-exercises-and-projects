{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8: matrix equations and linear least-squares fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix operations\n",
    "\n",
    "We begin today’s lab with a quick investigation of how to use numpy `array`s to represent matrices. We can use the `array` *constructor* function to make a matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
    "B = np.array([[2,0,0],[0,1,0],[0,0,1]])\n",
    "print(A)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is tempting to multiply the matrices by using `*`, but if we try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(A*B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Can you explain what has happened?** \n",
    "\n",
    "The correct way is to use the `dot` function, or to use the shorthand `@`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(A@B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.dot(A,B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy is clever enough to interpret a one-dimensional array as either a row or a column matrix depending on the context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = np.array([1,0,0])\n",
    "print(A@v)\n",
    "print(v@A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many useful matrix manipulation tools are in the `linalg` namespace. Among these, we can find the inverse of a matrix with `inv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.inv(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note as always that we can `import` functions directly into the main namespace if this is too much typing, by evaluating `from numpy.linalg import inv` or whatever.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s put this to use by solving a matrix equation. Specifically, let’s solve the simultaneous equations\n",
    "$$\\begin{align*}\n",
    "2x + 3y &= 16 \\\\\n",
    "7x - 2y &= 31 \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "or, equivalently,\n",
    "$$\n",
    "\\begin{pmatrix}2 & 3 \\\\ 7 & -2\\end{pmatrix}\n",
    "\\begin{pmatrix}x \\\\y \\end{pmatrix} =\n",
    "\\begin{pmatrix}16 \\\\ 31\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "The obvious way is to use the matrix inverse:\n",
    "$$\n",
    "\\begin{pmatrix}x \\\\y \\end{pmatrix} =\n",
    "\\begin{pmatrix}2 & 3 \\\\ 7 & -2\\end{pmatrix}^{-1}\n",
    "\\begin{pmatrix}16 \\\\ 31\\end{pmatrix}.\n",
    "$$\n",
    "**Use the matrix commands we’ve just learned to evaluate the RHS of this equation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option is to use the `linalg.solve` function, which exists specifically to solve matrix equations of the form $\\mathbf{Ax} = \\mathbf{b}$. It takes two arguments, the matrix $\\mathbf{A}$ and the vector $\\mathbf{b}$. **Check that you get the same answer using this method:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define suitable A and b here\n",
    "np.linalg.solve(M, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s compare the performance of these two commands on larger matrices. To generate a big matrix easily, we use the `rand` function, which lives in the `random` namespace in `numpy`. Calling `np.random.rand(20)` will return an `array` of 20 random numbers between 0 and 1, and similarly `np.random.rand(100,100)` will return a $100\\times 100$ `array`.\n",
    "\n",
    "Use the `%timeit` “magic” command to **compare the speeds of these two methods for matrices of suitable sizes**: you should be able to get up to at least $1000\\times 1000$. **Plot your results as a log-log graph.**\n",
    "\n",
    "(*Hint*: The easiest way to use this is just to put `%timeit` before the command you want to time. A more sophisticated method is to write something like\n",
    "\n",
    "    mytime = %timeit -o some_command()\n",
    "\n",
    "which will store a range of information about the timing in the variable `mytime`. For instance, the average time taken will be stored as `mytime.average`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "▶ **CHECKPOINT 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear least-squares fitting to polynomials\n",
    "\n",
    "First, we need some data to fit. The code below defines some example data; let's start by **plotting it**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([  0.        ,   0.52631579,   1.05263158,   1.57894737,\n",
    "                2.10526316,   2.63157895,   3.15789474,   3.68421053,\n",
    "                4.21052632,   4.73684211,   5.26315789,   5.78947368,\n",
    "                6.31578947,   6.84210526,   7.36842105,   7.89473684,\n",
    "                8.42105263,   8.94736842,   9.47368421,  10.        ])\n",
    "y = np.array([ -23.65721091,  129.96285988,  316.40610918,  435.59305751,\n",
    "               554.44335211,  614.02561442,  698.64885428,  792.92150483,\n",
    "               857.4523591 ,  886.46905968,  901.03562214,  873.93486904,\n",
    "               829.10300783,  784.39562338,  725.48999232,  609.56149829,\n",
    "               493.30269619,  360.81172505,  203.03897181,   -5.83555244])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw in class that the best-fitting polynomial of order $k$ to a given data set has coefficients $(a_0, a_1, \\dots, a_k)$ that satisfy the matrix equation\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "N & \\sum_i x_i & \\sum_i x_i^2 & \\dots & \\sum_i x_i^k \\\\\n",
    "\\sum_i x_i & \\sum_i x_i^2 & \\sum_i x_i^3 & \\dots & \\sum_i x_i^{k+1} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\sum_i x_i^k & \\sum_i x_i^{k+1} & \\sum_i x_i^{k+2} & \\dots & \\sum_i x_i^{2k} \\\\\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix} a_0 \\\\ a_1 \\\\ \\vdots \\\\ a_k\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix} \\sum_i y_i \\\\ \\sum_i x_iy_i \\\\ \\vdots \\\\ \\sum_i x_i^k y_i\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "**Write functions `coefficient_matrix(x, k)` and `constant_matrix(x, y, k)` to calculate, respectively, the square matrix on the LHS and the vector on the RHS.** Here `x` and `y` are 1D numpy `array`s of data, and as in the equation above `k` is the order of polynomial to fit to.\n",
    "\n",
    "*Hint:* We can use the fact that mathematical operations on `array`s are evaluated elementwise to our advantage. For instance, $\\sum_i x_i^2 y_i$ can be written in Python as `sum(x**2 * y)` – can you see why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coefficient_matrix(x, k):\n",
    "    \"\"\"Documentation string goes here\"\"\"\n",
    "    ...\n",
    "\n",
    "def constant_matrix(x, y, k):\n",
    "    \"\"\"Documentation string goes here\"\"\"\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with, let’s look for a quadratic solution, $k = 2$. **Use the `solve` function, together with the matrix functions you just defined, to find the coefficients of the best quadratic fit to these data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quadratic_coefficients = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make your life a bit easier, I’ve defined a Python function that will evaluate a polynomial with given coefficients. Check that you understand how the function below works. Using it and the coefficients you’ve just found, **plot the data together with your fit to them and the *residual* (*i.e.*, the difference between data and fit).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_fitted(x, coefficients):\n",
    "    return sum([c*x**i for i, c in enumerate(coefficients)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "▶ **CHECKPOINT 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need some measure of how good the fit is. One such measure is the *reduced chi-squared*, which is simply the sum of the squared residuals divided by the number of *degrees of freedom* – that is, the number of parameters we have fitted:\n",
    "\n",
    "$$\n",
    "\\chi^2_\\text{red} = \\frac{\\sum_i(y_\\text{fitted} - y_\\text{observed})^2}{k + 1}.\n",
    "$$\n",
    "\n",
    "**Write a function `chi_squared(x, y, coefficients)` to calculate the reduced chi-squared. Evaluate this for your quadratic fit.** \n",
    "\n",
    "*Hint:* if you can, try to make your function independent of the number of coefficients so that you can reuse it for a fit that is not quadratic. The denominator $k+1$ is the size of `coefficients`: use `coefficients.size` or `len(coefficients)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_squared(x, y, coefficients):\n",
    "    \"\"\"Documentation string here.\"\"\"\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_squared(x, y, quadratic_coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Try higher-order fits (cubic, quartic, …). How does the $\\chi^2$ value change?**\n",
    "\n",
    "▶ **CHECKPOINT 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
